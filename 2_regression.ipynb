{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of machine learning problem we will explore is called a regression problem. A regression problem is one in which we use a set of features (or independent variables) to try to predict a continuous output (e.g. a real valued number). By showing a model enough examples, the hope is that the model can be trained to predict the output value given just the set of features, where the prediction is as close to the real value as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Loading and Preprocessing\n",
    "\n",
    "For this regression tutorial we will use a dataset from UCI's machine learning repository ([link](http://archive.ics.uci.edu/ml/datasets/Auto+MPG)) concerning city-cycle fuel consumption in miles per gallon. We will use this dataset and the 7 independent variables to predict the target, miles per gallons, for different car make and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of being a built-in `sklearn` dataset, the `auto-mpg` dataset is stored in a `.csv` file that can be accessed from the UCI repository, so we'll use `pandas` to load in a local copy. This dataset will require some preprocessing, which we will do after performing some exploratory data analysis (EDA).\n",
    "\n",
    "First, let's import some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `auto-mpg` dataset using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chevrolet chevelle malibu</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick skylark 320</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plymouth satellite</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amc rebel sst</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford torino</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mpg  cylinders  displacement horsepower  weight  \\\n",
       "car name                                                                      \n",
       "chevrolet chevelle malibu  18.0          8         307.0        130    3504   \n",
       "buick skylark 320          15.0          8         350.0        165    3693   \n",
       "plymouth satellite         18.0          8         318.0        150    3436   \n",
       "amc rebel sst              16.0          8         304.0        150    3433   \n",
       "ford torino                17.0          8         302.0        140    3449   \n",
       "\n",
       "                           acceleration  model year  origin  \n",
       "car name                                                     \n",
       "chevrolet chevelle malibu          12.0          70       1  \n",
       "buick skylark 320                  11.5          70       1  \n",
       "plymouth satellite                 11.0          70       1  \n",
       "amc rebel sst                      12.0          70       1  \n",
       "ford torino                        10.5          70       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/auto-mpg.csv', index_col='car name')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the information for the variable types of each of the columns from the UCI machine learning repository's [website](https://archive.ics.uci.edu/ml/datasets/auto+mpg):\n",
    "1. **mpg**: continuous\n",
    "2. **cylinders**: multi-valued discrete\n",
    "3. **displacement**: continuous\n",
    "4. **horsepower**: continuous\n",
    "5. **weight**: continuous\n",
    "6. **acceleration**: continuous\n",
    "7. **model year**: multi-valued discrete\n",
    "8. **origin**: multi-valued discrete\n",
    "9. **car name**: string (unique for each instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a little more time to explore this dataset and perform any preprocessing necessary. One of the most important steps before we start any machine learning problem is to get a better understanding of the data at hand.\n",
    "\n",
    "First, we see that the original dataset has 398 and 9 columns (1 column to identify the unique cars, 1 column for the target variable, and 7 columns of indepedent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to check to see if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg             False\n",
       "cylinders       False\n",
       "displacement    False\n",
       "horsepower      False\n",
       "weight          False\n",
       "acceleration    False\n",
       "model year      False\n",
       "origin          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it doesn't seem like we are missing any values, but if we check the UCI repository, the documentation mentions there are indeed missing values. Further investigation into the data set description file provided by UCI tells us that the `horsepower` column has 6 missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Missing Attribute Values:  horsepower has 6 missing values\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/auto-mpg.names | grep 'missing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the unique values in the `horsepower` column below, we can see that all the values are string numerals except for `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['?', '98', '97', '96', '95', '94', '93', '92', '91', '90', '89',\n",
       "       '88', '87', '86', '85', '84', '83', '82', '81', '80', '79', '78',\n",
       "       '77', '76', '75', '74', '72', '71', '70', '69', '68', '67', '66',\n",
       "       '65', '64', '63', '62', '61', '60', '58', '54', '53', '52', '49',\n",
       "       '48', '46', '230', '225', '220', '215', '210', '208', '200', '198',\n",
       "       '193', '190', '180', '175', '170', '167', '165', '160', '158',\n",
       "       '155', '153', '152', '150', '149', '148', '145', '142', '140',\n",
       "       '139', '138', '137', '135', '133', '132', '130', '129', '125',\n",
       "       '122', '120', '116', '115', '113', '112', '110', '108', '107',\n",
       "       '105', '103', '102', '100'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['horsepower'].sort_values(ascending=False).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to handle these values, either by removing the rows that contain them completely, or by using some strategy to generate proxy values that provide some approximation of what the values may have been. \n",
    "\n",
    "In general, if we have a large dataset, it is okay to go ahead and drop a couple rows, but given that our dataset is small in this case, we will try to generate proxy values using a technique known as **imputation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fill the `?` values using imputation, we'll need to convert them as NaN (not a number) values, which is a common representation of missing data in python. We'll also convert the column variable type from strings to floats so that our imputer can calculate the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?', np.nan)\n",
    "data = data.astype({'horsepower': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ford pinto</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2046</td>\n",
       "      <td>19.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford maverick</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2875</td>\n",
       "      <td>17.0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>renault lecar deluxe</th>\n",
       "      <td>40.9</td>\n",
       "      <td>4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1835</td>\n",
       "      <td>17.3</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford mustang cobra</th>\n",
       "      <td>23.6</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2905</td>\n",
       "      <td>14.3</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>renault 18i</th>\n",
       "      <td>34.5</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2320</td>\n",
       "      <td>15.8</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amc concord dl</th>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3035</td>\n",
       "      <td>20.5</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mpg  cylinders  displacement  horsepower  weight  \\\n",
       "car name                                                                  \n",
       "ford pinto            25.0          4          98.0         NaN    2046   \n",
       "ford maverick         21.0          6         200.0         NaN    2875   \n",
       "renault lecar deluxe  40.9          4          85.0         NaN    1835   \n",
       "ford mustang cobra    23.6          4         140.0         NaN    2905   \n",
       "renault 18i           34.5          4         100.0         NaN    2320   \n",
       "amc concord dl        23.0          4         151.0         NaN    3035   \n",
       "\n",
       "                      acceleration  model year  origin  \n",
       "car name                                                \n",
       "ford pinto                    19.0          71       1  \n",
       "ford maverick                 17.0          74       1  \n",
       "renault lecar deluxe          17.3          80       2  \n",
       "ford mustang cobra            14.3          80       1  \n",
       "renault 18i                   15.8          81       2  \n",
       "amc concord dl                20.5          82       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['horsepower'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, before we perform any imputing or encoding of our variables, we'll want to split our dataset into `train` and `test` data. This will let us fit the preprocessing steps to only the `train` data and then apply it to both the `train` and `test` datasets. If we don't do this, our preprocessing steps have the potential to introduce certain patterns from our `test` data, which can lead to [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to: **set the random seed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 10\n",
    "np.random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While imputation is useful for features, it doesn't make sense to impute the output variable. Let's just remove any rows from the data with missing output variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(axis=0, subset=['mpg'], inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out there wasn't any missing data. Regardless, this is an important step to do just in case there is missing data!\n",
    "\n",
    "Now we can extract the output variable `mpg` from the `DataFrame` to make the `X` and `Y` variables. We use a capital `X` to denote it is a `matrix` or 2-D array, and use a lowercase `y` to denote that it is a `vector`, or 1-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 7), (398,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns='mpg')\n",
    "y = data['mpg'].astype(np.float64)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the train_test_split function to split the entire dataset into 80% `train` data and 20% `test` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTrain shape: (318, 7) YTrain shape: (318,) \n",
      "\n",
      "XTest shape: (80, 7) YTest shape: (80,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print('XTrain shape:', X_train_raw.shape, 'YTrain shape:', y_train_raw.shape, '\\n')\n",
    "print('XTest shape:', X_test_raw.shape, 'YTest shape:', y_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "Imputation is the name given to the preprocessing step that transforms missing values. Here we'll impute any missing values using the average, or mean, of all the data that does exist, as that's the best guess for a data point if all we have is the data itself. To do that we'll use the `SimpleImputer` to assign the mean to all missing values by fitting against the train data\n",
    "\n",
    "There are also other strategies that can be used to impute missing data ([see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "imputer.fit(X_train_raw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to actually transforming the train and test datasets, let's also fit a **One Hot Encoder** to transform our categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data Processing\n",
    "\n",
    "As we saw from the documentation, the `auto-mpg` dataset contains both categorical and continuous features, which will each need to be preprocessed in different ways. We'll want transform the categorical variables into indicator variables (which are either 0 or 1) using a technique known as one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's make a list of the categorical variable names to be transformed into indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>datsun 210</th>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datsun 210 mpg</th>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>honda civic</th>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford maverick</th>\n",
       "      <td>6</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volkswagen rabbit</th>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   cylinders  model year  origin\n",
       "car name                                        \n",
       "datsun 210                 4          79       3\n",
       "datsun 210 mpg             4          81       3\n",
       "honda civic                4          74       3\n",
       "ford maverick              6          73       1\n",
       "volkswagen rabbit          4          75       2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the variable names that are categorical for use later\n",
    "cat_var_names = ['cylinders', 'model year', 'origin']\n",
    "X_train_raw_cat = X_train_raw[cat_var_names]\n",
    "X_train_raw_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding (One-hot & Dummy)\n",
    "\n",
    "Many machine learning algorithms require that categorical data be encoded numerically in some fashion. A common technique used is called One-hot-encoding, which creates `k` new variables for a single categorical variable with `k` categories (or levels), where each new variable is coded with a `1` for the observations that contain that category, and a `0` for each observation that doesn't. \n",
    "\n",
    "However, when using some machine learning alorithms, such as linear regression, ridge regression and elastic net regression (which we will use first), we can run into the so-called [\"Dummy Variable Trap\"](https://www.algosome.com/articles/dummy-variable-trap-regression.html) when using One-Hot-Encoding on multiple categorical variables within the same set of features. This occurs because each set of one-hot-encoded variables can be added together across columns to create a single column of all `1`s, and so are multi-colinear when multiple one-hot-encoded variables exist within a given model. This can lead to misleading results when using the aforemetioned algorithms.\n",
    "\n",
    "To resolve this, we can simply add an intercept term to our model (which is all `1`s) and remove the first one-hot-encoded variable for each categorical variables, resulting in `k-1` so-called \"Dummy Variables\". \n",
    "\n",
    "Luckily the `OneHotEncoder` from `sklearn` can perform both one-hot and dummy encoding simply by setting the `drop` parameter. Let's use it to transform the `cylinders`, `model year`, and `origin` variables into `k-1` dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', handle_unknown='ignore', sparse=False)\n",
    "dummy_e.fit(X_train_raw_cat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the dummy encoder, there are 21 total unique values (or possible variables) among the categorical variables. After we apply the dummy encoder, this dimension will be reduced to 18 total unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 total unique values among the categorical variables\n"
     ]
    }
   ],
   "source": [
    "num_unique = sum([len(cat) for cat in dummy_e.categories_])\n",
    "print(f\"{num_unique} total unique values among the categorical variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Using `pandas`\n",
    "\n",
    "Optionally you can use `pandas` to do one-hot-encoding or dummy encoding. The problem with this, as we'll see in Day 3 of this workshop, is that we cannot include this into a `sklearn` pipeline, which will be a useful thing to do. Similar to the `OneHotEncoder`, we can set the optional parameter `drop_first` to change the behavior of the function from one-hot-encoding to dummy encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 7), (318, 22))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw_dummy = pd.get_dummies(X_train_raw, columns=cat_var_names, drop_first=True)\n",
    "X_train_raw.shape, X_train_raw_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Data Preprocessing\n",
    "\n",
    "Preprocessing continuous data requires different steps than categorical data. We'll still want to impute continuous data, but here we use the mean, median, or even more complex methods to make guesses at the missing data values. We don't need to create indicator variables, instead we need to normalize our variables, which helps improve performance of many machine learning models.\n",
    "\n",
    " Let's make subset out the continuous varialbles to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>datsun 210</th>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datsun 210 mpg</th>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>honda civic</th>\n",
       "      <td>120.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2489</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford maverick</th>\n",
       "      <td>250.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3021</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volkswagen rabbit</th>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1937</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   displacement  horsepower  weight  acceleration\n",
       "car name                                                         \n",
       "datsun 210                 85.0        65.0    2020          19.2\n",
       "datsun 210 mpg             85.0        65.0    1975          19.4\n",
       "honda civic               120.0        97.0    2489          15.0\n",
       "ford maverick             250.0        88.0    3021          16.5\n",
       "volkswagen rabbit          90.0        70.0    1937          14.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw_num = X_train_raw.drop(columns=cat_var_names)\n",
    "X_train_raw_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) is a transformation that puts data into some known \"normal\" scale. We use normalization to improve the performance of many machine learning algorithms (see [here](https://en.wikipedia.org/wiki/Feature_scaling)). There are many forms of normalization, but perhaps the most useful to machine learning algorithms is called the \"z-score\" also known as the standard score. \n",
    "\n",
    "To z-score normalize the data, we simply subtract the mean of the data, and divide by the standard deviation. This results in data with a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "We'll use the `StandardScaler` from `sklearn` to do normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 193.79716981,  104.22292994, 2980.69811321,   15.59559748]),\n",
       " array([1.09690854e+04, 1.50012228e+03, 7.23107248e+05, 7.90174162e+00]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm_e = StandardScaler()\n",
    "norm_e.fit(X_train_raw_num)\n",
    "norm_e.mean_, norm_e.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine it all together\n",
    "\n",
    "Now let's combine what we've learned to preprocess the entire dataset. On Day 3, we'll learn how to do this using an sklearn object called `Pipelines`. While these objects are extremely useful for preventing data leakage and having structured preprocessing, they require some set up, so we will use our preprocessors directly for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the `train` and `test` Input Data\n",
    "\n",
    "Becuase we've already fit our preprocessors on the train data, we can be safe in the knowledge that we can use them to transform both the train and test data without any data leakage.\n",
    "\n",
    "First, use the imputer to fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute the data\n",
    "X_train_imp = imputer.transform(X_train_raw)\n",
    "X_test_imp = imputer.transform(X_test_raw)\n",
    "\n",
    "# Check for missing values\n",
    "np.isnan(X_train_imp).any(), np.isnan(X_test_imp).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset out the categorical and numerical features separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical and numerical variable column indices\n",
    "feature_map = {idx:feat for idx, feat in enumerate(imputer.feature_names_in_)}\n",
    "cat_var_idx = [idx for idx, feat in feature_map.items() if feat in cat_var_names]\n",
    "num_var_idx = [idx for idx, feat in feature_map.items() if feat not in cat_var_names]\n",
    "\n",
    "# Splice the training array\n",
    "X_train_cat = X_train_imp[:, cat_var_idx]\n",
    "X_train_num = X_train_imp[:, num_var_idx]\n",
    "\n",
    "# Splice the test array\n",
    "X_test_cat = X_test_imp[:, cat_var_idx]\n",
    "X_test_num = X_test_imp[:, num_var_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the dummy encoder to the categorical variables and the normalizer to the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 18), (80, 18))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Categorical feature encoding\n",
    "X_train_dummy = dummy_e.transform(X_train_cat)\n",
    "X_test_dummy = dummy_e.transform(X_test_cat)\n",
    "\n",
    "X_train_dummy.shape, X_test_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 4), (80, 4))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numerical feature standardization\n",
    "X_train_norm = norm_e.transform(X_train_num)\n",
    "X_test_norm = norm_e.transform(X_test_num)\n",
    "\n",
    "X_train_norm.shape, X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, merge the categorical and numerical columns back into one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 22), (80, 22))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.hstack((X_train_dummy, X_train_norm))\n",
    "X_test = np.hstack((X_test_dummy, X_test_norm))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the `train` and `test` Outcome Variable\n",
    "\n",
    "Similarly to how we transformed the continous variables for the input data, we will want to do something similar for the outcome/dependent variable, `mpg`. Here, we'll use the `fit_transform` method on the train data which performs both the `fit` and `transform` steps in a single call, as we don't need to worry about any other prior fitting of preprocessors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_scaler = StandardScaler()\n",
    "y_train = mpg_scaler.fit_transform(y_train_raw.values.reshape(-1, 1))\n",
    "y_test = mpg_scaler.transform(y_test_raw.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, as soon as you have `X_train`, `X_test`, `y_train`, and `y_test`, everything else is just a matter of choosing your mdoel and the parameters for it. But this should not be trivialized, selecting models and that model's parameters is *very* important. While we will not cover it here, choosing the correct model and parameters is the core skill of applying machine learning algorithms, and can have dramatic affects on the performance of your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous machine learning models that can be used to model data and generate powerful predictions. These vary widely in the types of algorithms and statistical techniques that are used when building these models. Some models are purposefully built for regression problems, while others are more suited towards classification. Many models can also be used for both sets of problems with small tweaks to their algorithms.\n",
    "\n",
    "For our dataset, let's start with the most basic (and probably most common) regression model that exists: **Linear regression (or Ordinary Least Squares regression)**. Although fairly simple, linear regression is a very powerful model in its own right and can be effective when applied to certain regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, linear regression is nothing more than finding the best straight line, or line of best fit (hyperplane in multi-dimensional space), through a set of data points that most accurately captures the pattern that exists within those data points.\n",
    "\n",
    "In a univariate case (2-D), this looks something like this:\n",
    "\n",
    "![linear-regression](images/linear_regression_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multivariate case (3 dimensions or more), the line turns into a hyperplane which tries to capture as much of the information about the multi-dimensional data points as possible:\n",
    "\n",
    "![linear-regression](images/linear_regression_hyperplane.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general equation for a linear regression model is quite simple. All it includes are slope values (also known in machine learning as weights), or $\\beta$'s,  and an intercept (also known as a bias term), which is really just a special case of a weight, generally denoted as $\\beta_0$. The univariate equation is probably familiar to a lot of you:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Univariate regression:</center>\n",
    "\n",
    "$$y = b + mx $$\n",
    "$$Y = \\beta_0 + \\beta_1X_1$$\n",
    "<br>\n",
    "<center>Multivariate regression:</center>\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + ... + \\beta_iX_i$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to find a combination of these $\\beta_i$ values such that we pass through or as close to as many data points as possible. In other words, we are trying to find the values of $\\beta$ that reduce or minimize the aggregate distance between our linear model and the data points. \n",
    "\n",
    "We can formalize this into an optimization problem and pursue a strategy that is known in machine learning as minimizing the **cost function**. In the case of linear regression, the cost function we are trying to minimize is the **Mean Squared Error (MSE)** function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2 $$\n",
    "<br>\n",
    "<center>i: ith data point</center>\n",
    "\n",
    "<center>n: number of data points</center>\n",
    "<center>$Y_i$: The real value of the ith data point</center>\n",
    "<center>$\\hat{Y}_i$: The predicted value of the ith data point</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error is simply the sum of the squared errors (or distance) of each data point between the actual point in space and the predicted point from the linear model, all divided by the number of data points to get the mean.\n",
    "\n",
    "By minimizing this function, we will be able to find our optimal linear regression solution that best represents the patterns inherent within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Ordinary Least Squares (OLS) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start modeling with the basic [OLS linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) provided by scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression(n_jobs=1)  # CPUs to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done! Using scikit-learn to fit an linear regression model is as easy as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how well we fit the training set. For regression models, the `.score()` method returns the amount of variance in the output variable that can be explained by the model predictions. This is known as $R^2$, or R-squared. There are many other performance metrics that can be used when predicting continuous variables.\n",
    "\n",
    "Let's look at the $R^2$ for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.8772\n"
     ]
    }
   ],
   "source": [
    "print('Train R^2: %.04f' % (lin_reg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R^2: 0.8385\n"
     ]
    }
   ],
   "source": [
    "print('Test R^2: %.04f' % (lin_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common metric used in regression plots is the **Root Mean Squared Error (RMSE)**. This can be calculated by simply taking the square root of the MSE. In our case, we can intrepret this as the mean error made when predicting `mpg`, as RMSE is measured in the same units as the target variable.\n",
    "\n",
    "Here's the RMSE for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.3504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "train_pred = lin_reg.predict(X_train)\n",
    "test_pred = lin_reg.predict(X_test)\n",
    "\n",
    "print('Train RMSE: %.04f' % (mse(y_train, train_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.4030\n"
     ]
    }
   ],
   "source": [
    "print('Test RMSE: %.04f' % (mse(y_test, test_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.1228\n",
      "Test MSE: 0.1624\n"
     ]
    }
   ],
   "source": [
    "print('Train MSE: %.04f' % (mse(y_train, train_pred)))\n",
    "print('Test MSE: %.04f' % (mse(y_test, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final commonly used metric in regression is the **Mean Absolute Error (MAE)**. As the name suggests, this can be calculated by taking the mean of the absolute errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.2637\n",
      "Test MSE: 0.3151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "print('Train MSE: %.04f' % (mae(y_train, train_pred)))\n",
    "print('Test MSE: %.04f' % (mae(y_test, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Ridge (L2) Regression\n",
    "\n",
    "Many times, if we fit our models too closely to our training data, this can lead to a phenomenom called **overfitting**. It may seem like a good thing when we are able to match our data as close as possible, but often times there are differences in the data samples in our test set compared to our training set. To avoid this, most models are paired with some form of regularization (or penalization) that tries to account for unseen data in the test set. This may impact the performance on our training data, but can lead to better predictions on test data and improve overall generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For linear regression models, one form of regularization is known as **Ridge (L2) regression**. Instead of using the least squares loss (which is the loss function used to calculate our MSE cost function): \n",
    "$$ L(\\beta) = \\sum_i^n (y_i - \\hat y_i)^2 $$ \n",
    "\n",
    "In ridge regression we additionally penalize the coefficients by adding a regularization term: \n",
    "\n",
    "$$ L(\\beta) = \\sum_i^n (y_i - \\hat y_i)^2  + \\alpha \\sum_j^p \\beta^2 $$ \n",
    "\n",
    "This regularization term aims to minimize the size of any one coefficient (or weight), penalizing any reliance on a given subset of features which commonly leads to overfitting.\n",
    "\n",
    "Ridge regression takes a **hyperparameter**, called alpha, $\\alpha$ (sometimes lambda, $\\lambda$). This hyperparameter indicates how much regularization should be done. In other words, how much to care about the coefficient penalty term vs how much to care about the sum of squared errors term. The higher the value of alpha the more regularization, and the smaller the resulting coefficients will be. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) for more. \n",
    "\n",
    "If we use an `alpha` value of `0` then we get the same solution as the OLS regression done above. Let's prove that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0,  # regularization\n",
    "                  solver='auto',\n",
    "                  random_state = rand_seed) \n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ridge_train_pred = ridge_reg.predict(X_train)\n",
    "ridge_test_pred = ridge_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.3504\n",
      "Test RMSE: 0.4030\n"
     ]
    }
   ],
   "source": [
    "print('Train RMSE: %.04f' % (mse(y_train, ridge_train_pred, squared=False)))\n",
    "print('Test RMSE: %.04f' % (mse(y_test, ridge_test_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we don't know what the best value hypterparameter values should be, and so we need to leverage some type of trial and error method to determine the best values. We won't cover it today (it's covered in detail on Day 2), but scikit-learn provides a `RidgeCV` model that does just that. It fits a ridge regression model by first using cross-validation to find a good value of alpha. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for our sanity, let's see if we can improve on our baseline linear regression model using a ridge model by setting our alpha value to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha=0.1,  # regularization\n",
    "                  solver='auto',\n",
    "                  random_state = rand_seed) \n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ridge_train_pred = ridge_reg.predict(X_train)\n",
    "ridge_test_pred = ridge_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.3507\n",
      "Test RMSE: 0.4012\n"
     ]
    }
   ],
   "source": [
    "print('Train RMSE: %.04f' % (mse(y_train, ridge_train_pred, squared=False)))\n",
    "print('Test RMSE: %.04f' % (mse(y_test, ridge_test_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like despite doing slightly worse on the training set, it did a bit better than using regular OLS on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM - Lasso (L1) Regression\n",
    "\n",
    "**Lasso (L1) regression** is another form of regularized regression that penalizes the coefficients in a least squares loss. Rather than taking a squared penalty of the coefficients, Lasso uses an absolute value penalty: \n",
    "\n",
    "$$ L(\\beta) = \\sum_i^n (y_i - \\hat y_i)^2  + \\alpha \\sum_j^p |\\beta| $$ \n",
    "\n",
    "This has a similar effect on making the coefficients smaller, but also has a tendency to force some coefficients to 0. This leads to what is called **sparser** models, and is another way to reduce overfitting introduced by more complex models.\n",
    "\n",
    "See [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.01,  # regularization\n",
    "                  random_state = rand_seed) \n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "lasso_train_pred = lasso_reg.predict(X_train)\n",
    "lasso_test_pred = lasso_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.3916\n",
      "Test RMSE: 0.4333\n"
     ]
    }
   ],
   "source": [
    "print('Train RMSE: %.04f' % (mse(y_train, lasso_train_pred, squared=False)))\n",
    "print('Test RMSE: %.04f' % (mse(y_test, lasso_test_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that even with a small alpha, we have too much regularization which leads to worse performance on both train and test datasets. In this case, we would call our model **underfit**.\n",
    "\n",
    "Taking a look at our feature coeffiecients, we can see that many of them are 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02214324,  0.        , -0.29571066,  0.        , -0.00089397,\n",
       "       -0.19637668, -0.10984901, -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.01116894,  0.16207   ,  0.71175003,  0.43468887,\n",
       "        0.62432226,  0.        ,  0.11584334, -0.        , -0.21747105,\n",
       "       -0.50786204,  0.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Models: K-Nearest Neighbors (KNN)\n",
    "\n",
    "With more complex data, it may be difficult to capture model predictive linear relationships. In these cases, it can be useful to use models that are able to capture non-linear dependencies from the data.\n",
    "\n",
    "One such model is known as the **K-Nearest Neighbors (KNN)** algorithm. This algorithm is based off feature similarity, and uses data points that are similar to each other to predict the value of new data points. It does so by using a **distance metric** to quantify distance and therfore similarity between a set of points. In a KNN model, this distance metric can then be used to calculate an average value between `k` data points that are most similar to the data point to be predicted in the feature space.\n",
    "\n",
    "![KNN](images/KNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used distance metric for KNN is known as the **Eucliden distance**:\n",
    "\n",
    "$$ \\text{Euclidean distance} = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "By taking the average Eucliden distance of the `k` nearest points, we can derive a predicted value for a given data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding\n",
    "\n",
    "For the KNN model, we won't be fitting a bias term (or intercept), and so using dummy encoded categorical variables is inappropriate. Instead we'll use one-hot encoding for these models. Let's revisit our data preprocessing so that our data is in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories='auto', drop=None, handle_unknown='ignore', sparse=False)\n",
    "ohe.fit(X_train_raw_cat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 21), (80, 21))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Categorical feature encoding\n",
    "X_train_ohe = ohe.transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "X_train_ohe.shape, X_test_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 25), (80, 25))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nonlinear = np.hstack((X_train_ohe, X_train_norm))\n",
    "X_test_nonlinear = np.hstack((X_test_ohe, X_test_norm))\n",
    "\n",
    "X_train_nonlinear.shape, X_test_nonlinear.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbors regression\n",
    "\n",
    "Just like the linear regression models, scikit-learn provides a very easy interface to train a KNN model ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)). A quick look at the documentation gives away the fact that there are many more hyperparameters that can be altered compared to the previous models. KNN is a model that has much greater variability in performance based on these hyperparamters, so it is important that some **hyperparameter tuning** methods are applied to try combinations of different values. Again, we won't cover specific methods today, but it is an important point to remember when using KNN models in the future. \n",
    "\n",
    "Unlike linear regression models, a KNN model can be used for both regression and classification problems, so we should be sure to use the `KNNeighborsRegressor` class from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_k_neighbors(n_neighbors, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    for n in n_neighbors:\n",
    "        \n",
    "        knn_reg = KNeighborsRegressor(n_neighbors=n,\n",
    "                                      weights='uniform',  # distance weights points by inverse of their distance\n",
    "                                      algorithm='auto',  # out of ball_tree, kd_tree, brute\n",
    "                                      leaf_size=30)  # for tree algorithms\n",
    "        knn_reg.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        knn_train_pred = knn_reg.predict(X_train)\n",
    "        knn_test_pred = knn_reg.predict(X_test)\n",
    "        \n",
    "        print(\"WHEN n = %d\" % (n))\n",
    "        print('Train RMSE: %.04f' % (mse(y_train, knn_train_pred, squared=False)))\n",
    "        print('Test RMSE: %.04f' % (mse(y_test, knn_test_pred, squared=False)))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHEN n = 2\n",
      "Train RMSE: 0.2345\n",
      "Test RMSE: 0.4508\n",
      "\n",
      "WHEN n = 4\n",
      "Train RMSE: 0.3320\n",
      "Test RMSE: 0.3874\n",
      "\n",
      "WHEN n = 6\n",
      "Train RMSE: 0.3686\n",
      "Test RMSE: 0.3988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Example of hyperparameter tuning for the `k` neighbors value\n",
    "n_list = [2, 4, 6]\n",
    "tune_k_neighbors(n_list, X_train_nonlinear, y_train, X_test_nonlinear, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the performance varies greatly, but when n=4, we get our best performance yet! We can really see that more complex, non-linear models can oftentimes lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Another popular model that can be used for both classification and regression is a **Support Vector Machine (SVM)**. Using the datasets, train an SVM model and evaluate it's performance. See if you can also tweak the hyperparameters to inspect how the model varies in performance. Make sure to use `X_train` and `y_train` as your training inputs, and `X_test` as your test input.\n",
    "\n",
    "You can find the documentation for the [sklearn.svm.SVR here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR() # Add additional hyperparameters for the model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
