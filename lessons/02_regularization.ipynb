{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfcb6e0-d342-4a48-9812-4a1176599965",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Machine Learning: Regularization\n",
    "\n",
    "In machine learning, the name of the game is generalization. We want to have a model perform well on the training set, but we need to make sure that the patterns the model learns can actually generalize to data the model hasn't seen before.\n",
    "\n",
    "So, the scenario we want to avoid is that of **overfitting**. This occurs when our model too strongly learns patterns in the training data, and doesn't generalize well. Overfit models tend to exhibit large generalization gaps: large differences in predictive performance between the training and test data.\n",
    "\n",
    "Overfitting can happen for a variety of reasons, the most well known of which is having a model that's too complicated. Luckily, all is not lost. There are a variety of approaches we can use to combat overfitting. In general, these approaches are called **regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f833e6-4c67-4f64-8f00-83b848024c93",
   "metadata": {},
   "source": [
    "## Overfitting and Regularization\n",
    "\n",
    "In the previous lesson, we discussed feature engineering, the process by which we create new features in order to make our model more expressive. One tradeoff to adding features to the model is that the model becomes more complex, which makes it prone to overfitting. \n",
    "\n",
    "For example, consider a basic regression with the points shown below:\n",
    "\n",
    "![overfitting](../images/overfitting.png)\n",
    "\n",
    "We could fit a simple line to this data, which will exhibit some error. However, we could also fit a more complex model - say, a polynomial - which could perfectly fit to the training data. There will be no error in the training predictions, which seems great!\n",
    "\n",
    "But do we *really* think the polynomial is making good predictions on *all* possible data points? Look at how it behaves in between the training examples. It's very likely on *new* data - that is, when the model needs to generalize - the linear model will perform much better than the polynomial model. This is because the polynomial model overfit to the data.\n",
    "\n",
    "So, it's common in machine learning to follow a \"parsimony principle\". Specifically, we aim to choose simpler models that can still be predictive, because simpler models are less likely to overfit, and thus generalize decently well.\n",
    "\n",
    "Regularization is often though of in terms of the **bias-variance tradeoff**. Specifically, prediction errors often break down in terms of two components: bias and variance. The linear model exhibits higher bias, since it exhibits large errors on the training example. But the polynomial model has higher variance - it's more likely to give wildly different predictions for training samples close together.\n",
    "\n",
    "We don't always have to use linear regression in the spirit of opting for simpler models. Sometimes, it's good to use the complicated model, particularly if it makes sense in a specific context. This is where **regularization** is useful: a technique we can use to make a model less prone to overfitting during training. It's important to note that regularization is more of a concept than it is a specific, standardized technique. There are many approaches used for regularizing. Today, we're going to cover the usage of **penalty terms** to regularize linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08518305-c290-46bd-8998-c728d68ada22",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 1: Warm-Up\n",
    "\n",
    "Before we get started, let's warm up by importing our data and performing a train test split. We've providing the importing code for you. Go ahead and split the data into train/test sets using an 80/20 split, and a random state of 23.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8b441-f363-45ac-8abf-236df98f8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf9b24-e59a-41e9-8a82-2c2054363427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv('../data/auto-mpg.csv')\n",
    "# Remove the response variable and car name\n",
    "X = data.drop(columns=['car name', 'mpg'])\n",
    "# Assign response variable to its own variable\n",
    "y = data['mpg'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecbfb1-0121-42f7-81ab-180c2acd805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d536fb-a981-461f-a100-99dd469f6be4",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Recall the formulation of a linear model. We have the parameters we are trying to estimate, given in the model:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_P X_P$$\n",
    "\n",
    "We do this by minimizing the following objective function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MSE} = L(\\beta) &= \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\sum_{j=1}^P \\beta_j X_j\\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We're going to regularize this model. We're not going to change the actual linear model - that's the top equation - but we will change how we choose the $\\beta$ parameters. Specifically, we're going to do **ridge regression** (also called $\\ell_2$ regularization and Tikhonov regularization). Instead of using the least squares objective function, specified in the second equation, we're going to use the following objective function: \n",
    "\n",
    "$$ L(\\beta) = \\sum_{i=1}^N (y_i - \\hat y_i)^2  + \\alpha \\sum_{j=1}^P \\beta_j^2 $$ \n",
    "\n",
    "What's the difference? There's a second term added on, which is equal to the sum of the squares of the $\\beta$ values. What does this mean?\n",
    "\n",
    "Our goal is for the loss, $L(\\beta)$, to be as small as possible. The first term says we can make that small if we make our errors, $y_i - \\hat y_i$, small. The second term says that we increase the loss if the $\\beta$ values get too large. There's a tradeoff here: if we make the $\\beta$ values all zero to accomodate the second term, then the first term will be large. So, in ridge regression, we try and minimize the errors, while trying hard not to make the coefficients too big.\n",
    "\n",
    "Also, note that ridge regression requires a **hyperparameter**, called $\\alpha$ (sometimes $\\lambda$). This hyperparameter indicates how much regularization should be done. In other words, how much do we care about the coefficient penalty term vs. how much do we care about the sum of squared errors term? The higher the value of $\\alpha$, the more regularization, and the smaller the resulting coefficients will be. On the other hand, if we use an $\\alpha$ value of 0, we get the same solution as the OLS regression done above.\n",
    "\n",
    "Why does ridge regression serve as a good regularizer? The penalty actually does several things, which are beneficial for our model:\n",
    "1. **Multicollinearity:** Ridge regression was devised largely to combat multicollinearity, or when features are highly correlated with each other. Ordinary least squares struggles in these scenarios, because multicollinearity can cause a huge increase in variance: it makes the parameter estimates unstable. Adding the penalty term stabilizes the parameter estimates, at a little cost to bias. This results in better generalization performance.\n",
    "2. **Low Number of Samples:** The most common scenario where you might overfit is when you have many features, but not many samples. Adding the penalty term stabilizes the model in these scenarios. There's not a great intuition for this without diving into the math, so you can just take it at face value. \n",
    "3. **Shrinkage:** The $\\ell_2$ penalty results in shrinkage, or a small reduction in the size of the parameters. This is effectively a bias, but helps regularize by reducing variance that often comes with overfit models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107971a3-075d-4cc0-8fe3-c9fefe582ed0",
   "metadata": {},
   "source": [
    "## Ridge Regression in Practice\n",
    "\n",
    "As with linear regression, `scikit-learn` makes it easy to fit a ridge regression. We simply use the `Ridge` class from `scikit-learn`. This time, however, we're going to specify some arguments when we create the ridge regression object. The most important one is the regularization penalty, $\\alpha$, which we need to choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862640b7-4dee-487b-8308-d8c21aa61bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Create models\n",
    "ridge = Ridge(\n",
    "    # Regularization penalty\n",
    "    alpha=10,\n",
    "    random_state=1)\n",
    "# Fit object\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21a66c-d694-410e-a198-ac73df50eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "y_train_pred_ridge = ridge.predict(X_train)\n",
    "y_test_pred_ridge = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98614ab-de89-401f-b166-0eb216a2fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(f'Training R^2: {ridge.score(X_train, y_train)}')\n",
    "print(f'Test R^2: {ridge.score(X_test, y_test)}')\n",
    "print(f'Train RMSE: {mean_squared_error(y_train, y_train_pred_ridge, squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(y_test, y_test_pred_ridge, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c82e6e-a112-4c22-a048-6ca4c533a986",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 2: Benchmarking\n",
    "\n",
    "Re-run the ordinary least squares on the data using `LinearRegression`. Then, create a new ridge regression where the `alpha` penalty is set equal to zero. How do the performances of these models compare to each other? How do they compare with the original ridge regression? Be sure to compare both the training performances and test performances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e1c87-b0a7-44ef-ba8f-7246f97b57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# YOUR CODE HERE\n",
    "# Create models\n",
    "\n",
    "# Fit models\n",
    "\n",
    "# Run predictions\n",
    "\n",
    "# Evaluate models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5045a-a835-4de2-aa86-36d9511baef1",
   "metadata": {},
   "source": [
    "Based off your experiments, you probably found that ridge regression resulted in worse training performance, but slightly better generalization performance! So the regularization can help, particularly in this case where we know the parameters are correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8103a-e35e-4369-b694-1232e71f5981",
   "metadata": {},
   "source": [
    "## Choosing Hyperparameters: Validation Sets\n",
    "\n",
    "The current issue with our analysis thus far is that we don't know what $\\alpha$ value we should use. Since hyperparameters are chosen *before* we fit the model, we can't just choose them based off the training data. So, how should we go about conducting **hyperparameter search**: identifying the best hyperparameter(s) to use?\n",
    "\n",
    "Let's think back to our original goal. We want a model that generalizes to unseen data. So, ideally, the choice of the hyperparameter should be such that the performance on unseen data is the best. We can't use the test set for this, but what if we had another set of held-out data? \n",
    "\n",
    "This is the basis for a **validation set**. If we had extra held-out dataset, we could try a bunch of hyperparameters on the training set, and see which one results in a model that performs the best on the validation set. We then would choose that hyperparameter, and use it to refit the model on both the training data and validation data. We could then, finally, evaluate on the test set.\n",
    "\n",
    "![validation](../images/validation.png)\n",
    "\n",
    "So, you'll often see a dataset not only split up into training/test sets, but training/validation/test sets, particularly when you need to choose a hyperparameter.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "We just formulated the process of choosing a hyperparameter with a single validation set. However, there are many ways to perform validation. The most common way is **cross-validation**. Cross-validation is motivated by the concern that we may not choose the best hyperparameter if we're only validating on a small fraction of the data. If the validation sample, just by chance, contains specific data samples, we may bias our model in favor of those samples, and limit its generalizability.\n",
    "\n",
    "So, during cross-validation, we effectively validate on the *entire* dataset, by breaking it up into folds. Here's the process:\n",
    "\n",
    "1. Perform a train/test split, as you normally would.\n",
    "2. Choose a number of folds - the most common is $K=5$ - and split up your training data into those equally sized \"folds\".\n",
    "3. For *each* hyperparameter, we're going to fit $K$ models. Let's assume $K=5$. The first model will be fit on Folds 2-5, and validated on Fold 1. The second model will be fit on Folds 1, 3-5, and validated on Fold 2. This process continues for all 5 splits.\n",
    "4. Each hyperparameter's performance is summarized by the average predictive performance on all 5 held-out folds. We then choose the hyperparameter that had the best average performance.\n",
    "5. We can then refit a new model to the entire training set, using our chosen hyperparameter. That's our final model - evaluate it on the test set!\n",
    "\n",
    "![cross-validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1edb4d-60bf-45a6-a128-5f1097244cd5",
   "metadata": {},
   "source": [
    "## Cross-Validation in Practice\n",
    "\n",
    "You guessed it: `scikit-learn` makes it really easy to fit a model with cross-validation. We'll use the `RidgeCV` class. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) for details about it.\n",
    "\n",
    "`RidgeCV` is going to need to know a few things from us: which hyperparameters do we want? How many folds should we use? We'll specify these when creating the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f632ebb-b5ae-45aa-a7de-2139454ca79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "# Create ridge model, with CV\n",
    "ridge_cv = RidgeCV(\n",
    "    # Which alpha values to test for?\n",
    "    alphas=np.logspace(-1, 3, 100),\n",
    "    # Number of folds\n",
    "    cv=5)\n",
    "# Fit model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "# Evaluate model\n",
    "print(ridge_cv.score(X_train, y_train))\n",
    "print(ridge_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac60684-031b-43a3-b6f4-81f8b85e00da",
   "metadata": {},
   "source": [
    "We can also access the best $\\alpha$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3237a9d-7083-4681-a1de-b39dc6457a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf18376-c4a0-4f74-8f4d-d9d52b0ffede",
   "metadata": {},
   "source": [
    "As well as the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f569bf-38be-479d-8dea-5b8ef6e56ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a73083-aa63-4b5d-97c0-2ee8668993cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bonus Material: Lasso Regression\n",
    "\n",
    "**Lasso regression** (also called $\\ell_1$ regularization) is another form of regularized regression that penalizes the coefficients. Rather than taking a squared penalty of the coefficients, Lasso uses an absolute value penalty: \n",
    "\n",
    "$$ L(\\beta) = \\sum_{i=1}^N (y_i - \\hat y_i)^2  + \\alpha \\sum_{j=1}^P |\\beta_j| $$ \n",
    "\n",
    "This has a similar effect on making the coefficients smaller, but also has a tendency to force some coefficients to be set *exactly equal to 0*. This leads to what is called **sparser** models, and is another way to reduce overfitting introduced by more complex models.\n",
    "\n",
    "Setting some coefficients exactly equal to zero has the added benefit of performing **feature selection**: it can exactly identify if some features are not worth including in the model, because their coefficients are set exactly to 0 (meaning that their values would have no impact on prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f36182-7007-4403-8ad8-f79271a0bd54",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 3: Performing a Lasso Fit\n",
    "\n",
    "Below, we've imported the `Lasso` object from `scikit-learn` for you. Just like `Ridge`, it needs to know what the strength of the regularization penalty is before fitting to the data. \n",
    "\n",
    "Fit several Lasso models, with different regularization strengths. Try one with a small but non-zero regularization strength, and try one with a very large regularization strength. Look at the coefficients. What do you notice?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac5a53-31e1-4238-8314-ebedb5200079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
