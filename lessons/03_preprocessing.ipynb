{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dcf6e0-34d7-487a-afc7-0404106c4741",
   "metadata": {},
   "source": [
    "# Python Machine Learning: Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd8e28-1334-4520-b2d9-1b510ddb5819",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b25be6-01f4-4555-b8ae-66956d67ace5",
   "metadata": {},
   "source": [
    "Instead of being a built-in `sklearn` dataset, the `auto-mpg` dataset is stored in a `.csv` file that can be accessed from the UCI repository, so we'll use `pandas` to load in a local copy. This dataset will require some preprocessing, which we will do after performing some exploratory data analysis (EDA).\n",
    "\n",
    "First, let's import some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0142813-ac28-4ead-9996-39b2ada322ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612a6fb-fd37-4603-a430-2c018c5d7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/auto-mpg.csv', index_col='car name')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f79ca2-f223-4a2d-b5a1-edd1e2df3d96",
   "metadata": {},
   "source": [
    "Below is the information for the variable types of each of the columns from the UCI machine learning repository's [website](https://archive.ics.uci.edu/ml/datasets/auto+mpg):\n",
    "1. **mpg**: continuous\n",
    "2. **cylinders**: multi-valued discrete\n",
    "3. **displacement**: continuous\n",
    "4. **horsepower**: continuous\n",
    "5. **weight**: continuous\n",
    "6. **acceleration**: continuous\n",
    "7. **model year**: multi-valued discrete\n",
    "8. **origin**: multi-valued discrete\n",
    "9. **car name**: string (unique for each instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75343925-7865-43e6-bba1-f7fff9a673c1",
   "metadata": {},
   "source": [
    "## Missing Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8803cb-1e7b-43d6-bb35-e4a7422ea885",
   "metadata": {},
   "source": [
    "Let's take a little more time to explore this dataset and perform any preprocessing necessary. One of the most important steps before we start any machine learning problem is to get a better understanding of the data at hand.\n",
    "\n",
    "First, we see that the original dataset has 398 and 9 columns (1 column to identify the unique cars, 1 column for the target variable, and 7 columns of indepedent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174de64b-4d9f-47b0-8872-68ca268d6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e79231-28ea-49fc-be19-1282916c88b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Missing values\n",
    "\n",
    "Next, we want to check to see if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb04bc-4a44-493f-85d6-739adb1c7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe64b8-4686-43ef-b2f0-586974a74d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['horsepower'].sort_values(ascending=False).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938e029-aae5-4332-9719-cc26b9eba756",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?', np.nan)\n",
    "data = data.astype({'horsepower': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d5257-2d41-4f2a-8c05-53a9f204dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['horsepower'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bbd99-c5ba-474b-a194-0003ae520a04",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "Imputation is the name given to the preprocessing step that transforms missing values. Here we'll impute any missing values using the average, or mean, of all the data that does exist, as that's the best guess for a data point if all we have is the data itself. To do that we'll use the `SimpleImputer` to assign the mean to all missing values by fitting against the train data\n",
    "\n",
    "There are also other strategies that can be used to impute missing data ([see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30fe06-eb35-48af-88a2-b4cbd74e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "imputer.fit(X_train_raw);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e3fff-ded3-4c7a-9dfe-a3b9ff62a566",
   "metadata": {},
   "source": [
    "## Categorical Data Processing\n",
    "\n",
    "As we saw from the documentation, the `auto-mpg` dataset contains both categorical and continuous features, which will each need to be preprocessed in different ways. We'll want transform the categorical variables into indicator variables (which are either 0 or 1) using a technique known as one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bc33e-2b97-4b31-83d1-985dec1e5950",
   "metadata": {},
   "source": [
    " Let's make a list of the categorical variable names to be transformed into indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113d6a3-474c-4b57-9804-8040c38117a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable names that are categorical for use later\n",
    "cat_var_names = ['cylinders', 'model year', 'origin']\n",
    "X_train_raw_cat = X_train_raw[cat_var_names]\n",
    "X_train_raw_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2d7ba-036f-49e2-ab9e-dc06086eaed6",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding (One-hot & Dummy)\n",
    "\n",
    "Many machine learning algorithms require that categorical data be encoded numerically in some fashion. A common technique used is called One-hot-encoding, which creates `k` new variables for a single categorical variable with `k` categories (or levels), where each new variable is coded with a `1` for the observations that contain that category, and a `0` for each observation that doesn't. \n",
    "\n",
    "However, when using some machine learning alorithms, such as linear regression, ridge regression and elastic net regression (which we will use first), we can run into the so-called [\"Dummy Variable Trap\"](https://www.algosome.com/articles/dummy-variable-trap-regression.html) when using One-Hot-Encoding on multiple categorical variables within the same set of features. This occurs because each set of one-hot-encoded variables can be added together across columns to create a single column of all `1`s, and so are multi-colinear when multiple one-hot-encoded variables exist within a given model. This can lead to misleading results when using the aforemetioned algorithms.\n",
    "\n",
    "To resolve this, we can simply add an intercept term to our model (which is all `1`s) and remove the first one-hot-encoded variable for each categorical variables, resulting in `k-1` so-called \"Dummy Variables\". \n",
    "\n",
    "Luckily the `OneHotEncoder` from `sklearn` can perform both one-hot and dummy encoding simply by setting the `drop` parameter. Let's use it to transform the `cylinders`, `model year`, and `origin` variables into `k-1` dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9384a9e-453f-4b62-8bbf-7866b8ac441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', handle_unknown='ignore', sparse=False)\n",
    "dummy_e.fit(X_train_raw_cat);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08bf6c-e90c-42a2-aec6-1ddea260a170",
   "metadata": {},
   "source": [
    "Before using the dummy encoder, there are 21 total unique values (or possible variables) among the categorical variables. After we apply the dummy encoder, this dimension will be reduced to 18 total unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4091b24-0e57-47e3-a58a-d88826ab5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique = sum([len(cat) for cat in dummy_e.categories_])\n",
    "print(f\"{num_unique} total unique values among the categorical variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd0aeb-83ea-40e9-a662-6fc8b75469be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [OPTIONAL] Using `pandas`\n",
    "\n",
    "Optionally you can use `pandas` to do one-hot-encoding or dummy encoding. The problem with this, as we'll see in Day 3 of this workshop, is that we cannot include this into a `sklearn` pipeline, which will be a useful thing to do. Similar to the `OneHotEncoder`, we can set the optional parameter `drop_first` to change the behavior of the function from one-hot-encoding to dummy encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcadf01-6c78-45f8-a0c1-cc1e55a2aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw_dummy = pd.get_dummies(X_train_raw, columns=cat_var_names, drop_first=True)\n",
    "X_train_raw.shape, X_train_raw_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec19bc9-6aee-48d1-b043-04ab71e4208b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Continuous Data Preprocessing\n",
    "\n",
    "Preprocessing continuous data requires different steps than categorical data. We'll still want to impute continuous data, but here we use the mean, median, or even more complex methods to make guesses at the missing data values. We don't need to create indicator variables, instead we need to normalize our variables, which helps improve performance of many machine learning models.\n",
    "\n",
    " Let's make subset out the continuous varialbles to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06511352-4ba4-4bb5-8da4-82430ac080a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_raw_num = X_train_raw.drop(columns=cat_var_names)\n",
    "X_train_raw_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13162f8-71d0-4f34-8edb-2b95516b4fa0",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) is a transformation that puts data into some known \"normal\" scale. We use normalization to improve the performance of many machine learning algorithms (see [here](https://en.wikipedia.org/wiki/Feature_scaling)). There are many forms of normalization, but perhaps the most useful to machine learning algorithms is called the \"z-score\" also known as the standard score. \n",
    "\n",
    "To z-score normalize the data, we simply subtract the mean of the data, and divide by the standard deviation. This results in data with a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "We'll use the `StandardScaler` from `sklearn` to do normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f872ea-59e4-46a6-b366-578f6d0716a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm_e = StandardScaler()\n",
    "norm_e.fit(X_train_raw_num)\n",
    "norm_e.mean_, norm_e.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7c3bf-c215-4de8-830d-c933ed52c505",
   "metadata": {},
   "source": [
    "## Combine it all together\n",
    "\n",
    "Now let's combine what we've learned to preprocess the entire dataset. On Day 3, we'll learn how to do this using an sklearn object called `Pipelines`. While these objects are extremely useful for preventing data leakage and having structured preprocessing, they require some set up, so we will use our preprocessors directly for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26159591-d6ad-47fd-9e7d-078029babc76",
   "metadata": {},
   "source": [
    "### Transform the `train` and `test` Input Data\n",
    "\n",
    "Becuase we've already fit our preprocessors on the train data, we can be safe in the knowledge that we can use them to transform both the train and test data without any data leakage.\n",
    "\n",
    "First, use the imputer to fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b944a-fb32-4d24-8e54-b81d921b7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the data\n",
    "X_train_imp = imputer.transform(X_train_raw)\n",
    "X_test_imp = imputer.transform(X_test_raw)\n",
    "\n",
    "# Check for missing values\n",
    "np.isnan(X_train_imp).any(), np.isnan(X_test_imp).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2be342-483d-4d5b-b3ba-105b60e2cfeb",
   "metadata": {},
   "source": [
    "Subset out the categorical and numerical features separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05022a-a041-4d01-b189-5ceb5e1e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical and numerical variable column indices\n",
    "feature_map = {idx:feat for idx, feat in enumerate(imputer.feature_names_in_)}\n",
    "cat_var_idx = [idx for idx, feat in feature_map.items() if feat in cat_var_names]\n",
    "num_var_idx = [idx for idx, feat in feature_map.items() if feat not in cat_var_names]\n",
    "\n",
    "# Splice the training array\n",
    "X_train_cat = X_train_imp[:, cat_var_idx]\n",
    "X_train_num = X_train_imp[:, num_var_idx]\n",
    "\n",
    "# Splice the test array\n",
    "X_test_cat = X_test_imp[:, cat_var_idx]\n",
    "X_test_num = X_test_imp[:, num_var_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b746b78-8d31-40e9-819e-2273278c2f88",
   "metadata": {},
   "source": [
    "Apply the dummy encoder to the categorical variables and the normalizer to the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d20a3-73b9-490c-9f81-23e37fc09a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Categorical feature encoding\n",
    "X_train_dummy = dummy_e.transform(X_train_cat)\n",
    "X_test_dummy = dummy_e.transform(X_test_cat)\n",
    "\n",
    "X_train_dummy.shape, X_test_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c7fc4-fd8e-4deb-832a-8e02d82909d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical feature standardization\n",
    "X_train_norm = norm_e.transform(X_train_num)\n",
    "X_test_norm = norm_e.transform(X_test_num)\n",
    "\n",
    "X_train_norm.shape, X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309dc2b-bdf8-420c-a3f3-fe93c854c3eb",
   "metadata": {},
   "source": [
    "Finally, merge the categorical and numerical columns back into one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97ace9-bd20-49c0-bae9-bd629a8b7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train_dummy, X_train_norm))\n",
    "X_test = np.hstack((X_test_dummy, X_test_norm))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da1658-0d6d-4881-b56a-f3cb0073044f",
   "metadata": {},
   "source": [
    "### Transform the `train` and `test` Outcome Variable\n",
    "\n",
    "Similarly to how we transformed the continous variables for the input data, we will want to do something similar for the outcome/dependent variable, `mpg`. Here, we'll use the `fit_transform` method on the train data which performs both the `fit` and `transform` steps in a single call, as we don't need to worry about any other prior fitting of preprocessors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced964b-1672-421a-8d47-51396611224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_scaler = StandardScaler()\n",
    "y_train = mpg_scaler.fit_transform(y_train_raw.values.reshape(-1, 1))\n",
    "y_test = mpg_scaler.transform(y_test_raw.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4ecff-fb89-4f71-a7ef-70aa43ccc691",
   "metadata": {},
   "source": [
    "In scikit-learn, as soon as you have `X_train`, `X_test`, `y_train`, and `y_test`, everything else is just a matter of choosing your mdoel and the parameters for it. But this should not be trivialized, selecting models and that model's parameters is *very* important. While we will not cover it here, choosing the correct model and parameters is the core skill of applying machine learning algorithms, and can have dramatic affects on the performance of your predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
