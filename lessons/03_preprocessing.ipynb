{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dcf6e0-34d7-487a-afc7-0404106c4741",
   "metadata": {},
   "source": [
    "# Python Machine Learning: Preprocessing\n",
    "\n",
    "Preprocessing is an essential step of the machine learning workflow and important for the performance of models. This notebook will introduce the major steps of preprocessing for machine learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd8e28-1334-4520-b2d9-1b510ddb5819",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b25be6-01f4-4555-b8ae-66956d67ace5",
   "metadata": {},
   "source": [
    "For today, we will be working with the `penguins` data set. This data set is from [Kaggle](https://www.kaggle.com/parulpandey/penguin-dataset-the-new-iris) and includes some penguins of three different species, their location, and some measurements for each penguin.\n",
    "\n",
    "First, let's import some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0142813-ac28-4ead-9996-39b2ada322ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769ae58",
   "metadata": {},
   "source": [
    "Now, let's load in the data from the `data` subfolder of this directory.\n",
    "\n",
    "**Question:** How many columns are there in this data set? How many rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612a6fb-fd37-4603-a430-2c018c5d7f29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/penguins.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f79ca2-f223-4a2d-b5a1-edd1e2df3d96",
   "metadata": {},
   "source": [
    "Below is the information for each of the columns:\n",
    "1. **species**: Species of penguin [Adelie, Chinstrap, Gentoo]\n",
    "2. **island**: Island where the penguin was found [Torgersen, Biscoe]\n",
    "3. **culmen_length_mm**: Length of upper part of penguin's bill (millimeters)\n",
    "4. **culmen_depth_mm**: Height of upper part of bill (millimeters)\n",
    "5. **flipper_length_mm**: Length of penguin flipper (millimeters)\n",
    "6. **body_mass_g**: Body mass of the penguin (grams)\n",
    "7. **sex**: Biological sex of the penguin [MALE, FEMALE]\n",
    "\n",
    "\n",
    "**Question:** Which of the columns are continuous? Which are categorical?\n",
    "\n",
    "\n",
    "We will need to treat the numeric and categorical data differently in preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75343925-7865-43e6-bba1-f7fff9a673c1",
   "metadata": {},
   "source": [
    "## Missing Data Preprocessing\n",
    "\n",
    "First, let's check to see if there are any missing values in the data set. Missing values are represented by `NaN`. \n",
    "\n",
    "**Question:** In this case, what do missing values stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb04bc-4a44-493f-85d6-739adb1c7d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd318fc2",
   "metadata": {},
   "source": [
    "It is also possible to have non `NaN` missing values. For example, let's take a look at the `sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d613dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed852c0",
   "metadata": {},
   "source": [
    "In this case, the `.` represents a missing value, so let's replace those with `np.nan` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('.', np.nan, inplace=True)\n",
    "\n",
    "data['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bbd99-c5ba-474b-a194-0003ae520a04",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "In the case of missing values, we have the option to fill in the missing values with the best guess. This is called **imputation**. Here we'll impute any missing values using the average, or mean, of all the data that does exist, as that's the best guess for a data point if all we have is the data itself. To do that we'll use the `SimpleImputer` to assign the mean to all missing values in the data.\n",
    "\n",
    "There are also other strategies that can be used to impute missing data ([see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)).\n",
    "\n",
    "Let's see how the `SimpleImputer` works on a subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30fe06-eb35-48af-88a2-b4cbd74e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "imputed = imputer.fit_transform(data[['body_mass_g','flipper_length_mm']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e085f8",
   "metadata": {},
   "source": [
    "Now let's check that the previously null values have been filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7157f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(imputed[data[data['body_mass_g'].isna()].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de080754",
   "metadata": {},
   "source": [
    "### Dropping Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f21878",
   "metadata": {},
   "source": [
    "Another option option is to use `pd.dropna()` to drop `Null` values from the `DataFrame`. This should almost always be used with the `subset` argument which restricts the function to only dropping values that are null in a certain column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['sex'])\n",
    "\n",
    "# Now this line will return an empty dataframe\n",
    "data[data['sex'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e3fff-ded3-4c7a-9dfe-a3b9ff62a566",
   "metadata": {},
   "source": [
    "## Categorical Data Processing\n",
    "\n",
    "As we saw earlier, the `penguins` dataset contains both categorical and continuous features, which will each need to be preprocessed in different ways. First, we want to transform the categorical variables from strings to **indicator variables**. Indicator variables have one column per level, For example, the island variable will change from Biscoe/Dream/Torgersen --> Biscoe (1/0), Dream (1/0), and Torgerson (1/0). For each set of indicator variables, there should be a 1 in exactly one column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bc33e-2b97-4b31-83d1-985dec1e5950",
   "metadata": {},
   "source": [
    " Let's make a list of the categorical variable names to be transformed into indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113d6a3-474c-4b57-9804-8040c38117a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable names that are categorical for use later\n",
    "cat_var_names = ['island', 'sex']\n",
    "data_cat = data[cat_var_names]\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2d7ba-036f-49e2-ab9e-dc06086eaed6",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding (One-hot & Dummy)\n",
    "\n",
    "Many machine learning algorithms require that categorical data be encoded numerically in some fashion. There are two main ways to do so:\n",
    "\n",
    "\n",
    "- **One-hot-encoding**, which creates `k` new variables for a single categorical variable with `k` categories (or levels), where each new variable is coded with a `1` for the observations that contain that category, and a `0` for each observation that doesn't. \n",
    "- **Dummy encoding**, which creates `k-1` new variables for a categorical variable with `k` categories\n",
    "\n",
    "However, when using some machine learning algorithms we can run into the so-called [\"Dummy Variable Trap\"](https://www.algosome.com/articles/dummy-variable-trap-regression.html) when using One-Hot-Encoding on multiple categorical variables within the same set of features. This occurs because each set of one-hot-encoded variables can be added together across columns to create a single column of all `1`s, and so are multi-colinear when multiple one-hot-encoded variables exist within a given model. This can lead to misleading results. \n",
    "\n",
    "To resolve this, we can simply add an intercept term to our model (which is all `1`s) and remove the first one-hot-encoded variable for each categorical variables, resulting in `k-1` so-called \"Dummy Variables\". \n",
    "\n",
    "Luckily the `OneHotEncoder` from `sklearn` can perform both one-hot and dummy encoding simply by setting the `drop` parameter (`drop = 'first'` for Dummy Encoding and `drop = None` for One Hot Encoding). \n",
    "\n",
    "**Question:** How many total columns will there be in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9384a9e-453f-4b62-8bbf-7866b8ac441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', sparse=False)\n",
    "dummy_e.fit(data_cat);\n",
    "dummy_e.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4091b24-0e57-47e3-a58a-d88826ab5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dummy_e.transform(data_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec19bc9-6aee-48d1-b043-04ab71e4208b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Continuous Data Preprocessing\n",
    "\n",
    "For numeric data, we don't need to create indicator variables, instead we need to normalize our variables, which helps improve performance of many machine learning models.\n",
    "\n",
    " Let's make subset out the continuous variables to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06511352-4ba4-4bb5-8da4-82430ac080a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_num = data.drop(columns=cat_var_names + ['species'])\n",
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13162f8-71d0-4f34-8edb-2b95516b4fa0",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) is a transformation that puts data into some known \"normal\" scale. We use normalization to improve the performance of many machine learning algorithms (see [here](https://en.wikipedia.org/wiki/Feature_scaling)). There are many forms of normalization, but perhaps the most useful to machine learning algorithms is called the \"z-score\" also known as the standard score. \n",
    "\n",
    "To z-score normalize the data, we simply subtract the mean of the data, and divide by the standard deviation. This results in data with a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "We'll use the `StandardScaler` from `sklearn` to do normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f872ea-59e4-46a6-b366-578f6d0716a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm_e = StandardScaler()\n",
    "norm_e.fit_transform(data_num,).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c20c9",
   "metadata": {},
   "source": [
    "To check the normalization works, let's look at the mean and standard variation of the resulting columns. \n",
    "\n",
    "**Question:** What should the mean and std variation be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean:',norm_e.fit_transform(data_num,).mean(axis=0))\n",
    "print('std:',norm_e.fit_transform(data_num,).std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c54f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge 1: Fitting preprocessing functions\n",
    "\n",
    "The simple imputer, normalization and one-hot-encoding rely on sklearn functions that are fit to a data set. \n",
    "\n",
    "1) What is being fit for each of the three functions?\n",
    "    1) One Hot Encoding\n",
    "    2) Standard Scaler\n",
    "    3) Simple Imputer\n",
    "    \n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "When we are preprocessing data we have a few options: \n",
    "1) Fit on the whole data set\n",
    "2) Fit on the training data\n",
    "3) Fit on the testing data\n",
    "\n",
    "Which of the above methods would you use and why?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7c3bf-c215-4de8-830d-c933ed52c505",
   "metadata": {},
   "source": [
    "## Combine it all together\n",
    "\n",
    "Now let's combine what we've learned to preprocess the entire dataset.\n",
    "\n",
    "First we will reload the data set to start with a clean copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b097530",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/penguins.csv')\n",
    "data.replace('.', np.nan, inplace=True)\n",
    "data = data.dropna(subset=['sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split\n",
    "y = data['species']\n",
    "X = data.drop('species', axis =1, inplace=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=.25, stratify=y)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadb45c",
   "metadata": {},
   "source": [
    "We want to train our imputers on the training data using `fit_transform`, then `transform` the test data. This more closely resembles what the workflow would look like if you are bringing in brand new test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2be342-483d-4d5b-b3ba-105b60e2cfeb",
   "metadata": {},
   "source": [
    "First, we will subset out the categorical and numerical features separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05022a-a041-4d01-b189-5ceb5e1e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical and numerical variable column indices\n",
    "cat_var = ['island', 'sex']\n",
    "num_var = ['culmen_length_mm', 'culmen_depth_mm',\n",
    "           'flipper_length_mm', 'body_mass_g']\n",
    "# Splice the training array\n",
    "X_train_cat = X_train[cat_var]\n",
    "X_train_num = X_train[num_var]\n",
    "\n",
    "# Splice the test array\n",
    "X_test_cat = X_test[cat_var]\n",
    "X_test_num = X_test[num_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b746b78-8d31-40e9-819e-2273278c2f88",
   "metadata": {},
   "source": [
    "Now, let's process the categorical data with **Dummy encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d20a3-73b9-490c-9f81-23e37fc09a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Categorical feature encoding\n",
    "X_train_dummy = dummy_e.fit_transform(X_train_cat)\n",
    "X_test_dummy = dummy_e.transform(X_test_cat)\n",
    "\n",
    "\n",
    "# Check the shape\n",
    "X_train_dummy.shape, X_test_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae07768",
   "metadata": {},
   "source": [
    "Now, let's process the numerical data by imputing any missing values and normalizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c7fc4-fd8e-4deb-832a-8e02d82909d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical feature standardization\n",
    "\n",
    "# Impute the data\n",
    "X_train_imp = imputer.fit_transform(X_train_num)\n",
    "X_test_imp = imputer.transform(X_test_num)\n",
    "\n",
    "# Check for missing values\n",
    "np.isnan(X_train_imp).any(), np.isnan(X_test_imp).any()\n",
    "\n",
    "# normalize\n",
    "X_train_norm = norm_e.fit_transform(X_train_num)\n",
    "X_test_norm = norm_e.transform(X_test_num)\n",
    "\n",
    "X_train_norm.shape, X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309dc2b-bdf8-420c-a3f3-fe93c854c3eb",
   "metadata": {},
   "source": [
    "Now that we've processed the numerical and categorical data separately, we can put the two arrays back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97ace9-bd20-49c0-bae9-bd629a8b7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train_dummy, X_train_norm))\n",
    "X_test = np.hstack((X_test_dummy, X_test_norm))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab00968",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge 2: Order of Preprocessing\n",
    "\n",
    "In the preprocessing we did the following steps: \n",
    "\n",
    "1) Null values\n",
    "2) One-hot-encoding\n",
    "3) Imputation\n",
    "4) Normalization\n",
    "\n",
    "Now, consider that we change the order of the steps in the following ways. What effect might that have on the algorithms?\n",
    "**Hint**: Try copying the code from above and trying it out!\n",
    "\n",
    "- One-Hot-Encoding before Null Values\n",
    "- Normalization before Null values\n",
    "\n",
    "**Bonus:** Are there any other switches in order that might affect preprocessing?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4ecff-fb89-4f71-a7ef-70aa43ccc691",
   "metadata": {},
   "source": [
    "Finally, let's save our results as separate `.csv` files, so we won't have to run the preprocessing again.\n",
    "\n",
    "First we will make them DataFrames, add columns, and save them as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = ['Dream','Torgersen', 'Male',\n",
    "                   'culmen_length_mm', 'culmen_depth_mm',\n",
    "                   'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "X_test.columns = ['Dream','Torgersen', 'Male',\n",
    "                   'culmen_length_mm', 'culmen_depth_mm',\n",
    "                   'flipper_length_mm', 'body_mass_g']\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = ['species']\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.columns = ['species']\n",
    "\n",
    "X_train.to_csv('../data/penguins_X_train.csv')\n",
    "X_test.to_csv('../data/penguins_X_test.csv')\n",
    "y_train.to_csv('../data/penguins_y_train.csv')\n",
    "y_test.to_csv('../data/penguins_y_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6de745",
   "metadata": {},
   "source": [
    "Although now we will move on to talk about classification, all of the choices we make in the preprocessing pipeline are extremely important to machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06995721",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge 3: Preprocessing and regularization\n",
    "\n",
    "We are preprocessing data in preparation for a classification task down the line. However, preprocessing also applies to regression. \n",
    "\n",
    "Consider the regularization task applied in the previous notebook. How might the preprocessing steps affect the performance of regularization?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0895317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
