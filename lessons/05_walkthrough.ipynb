{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning: Machine Learning Walkthrough\n",
    "\n",
    "In this notebook, we're going to execute a machine learning project from start to finish. We'll use techniques covered in the previous workshops to facilitate this process, but we'll also introduce some new concepts. Our goal is to demonstrate a basic machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Pipeline\n",
    "\n",
    "We'll take the following steps to develop our machine learning models:\n",
    "\n",
    "1. **Introduce Dataset and Objectives**\n",
    "    - What is the dataset?\n",
    "    - What do the features tell us?\n",
    "    - What is our goal in applying machine learning here? \n",
    "    - Why is classification important in this scenario?\n",
    "2. **Exploratory Data Analysis and Feature Engineering**\n",
    "    - Produce several plots to give us a better understanding of the data.\n",
    "    - Perform feature engineering and preprocessing.\n",
    "    - Create a validation dataset from the data and set it aside until the end when evaluate final models.\n",
    "3. **Modeling Process**\n",
    "    - Determine the null accuracy and preferred performance metric.\n",
    "    - Train three different models: Logistic Regression, Decision Trees and Random Forest.\n",
    "4. **Evaluation and Interpretation**\n",
    "    - Evaluate the model using a variety of metrics.\n",
    "    - Discuss how successful we were with our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduce Dataset and Objectives\n",
    "\n",
    "We are going to be using a Kaggle dataset called [\"Personal Key Indicators of Heart Disease\"](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease).\n",
    "\n",
    "This dataset consists of 2020 annual CDC survey data from 400,000 adults related to their health status with regard to heart disease.\n",
    "\n",
    "Below, we provide an edited down description of the data, taken from the Kaggle data description:\n",
    "\n",
    "### What topic does the dataset cover?\n",
    "\n",
    "According to the CDC, heart disease is one of the leading causes of death for people of most races in the US (African Americans, American Indians and Alaska Natives, and white people). About half of all Americans (47%) have at least 1 of 3 key risk factors for heart disease: high blood pressure, high cholesterol, and smoking. Other key indicator include diabetic status, obesity (high BMI), not getting enough physical activity or drinking too much alcohol. Detecting and preventing the factors that have the greatest impact on heart disease is very important in healthcare. Computational developments, in turn, allow the application of machine learning methods to detect \"patterns\" from the data that can predict a patient's condition.\n",
    "\n",
    "### Where did the dataset come from and what treatments did it undergo?\n",
    "\n",
    "Originally, the dataset came from the CDC and is a major part of the Behavioral Risk Factor Surveillance System (BRFSS), which conducts annual telephone surveys to gather data on the health status of U.S. residents. The most recent dataset (as of February 15, 2022) includes data from 2020. It consists of 401,958 rows and 279 columns.\n",
    "\n",
    "The vast majority of columns are questions asked to respondents about their health status, such as \"Do you have serious difficulty walking or climbing stairs?\" or \"Have you smoked at least 100 cigarettes in your entire life?\".\n",
    "\n",
    "In this dataset, I noticed many different factors (questions) that directly or indirectly influence heart disease, so I decided to select the most relevant variables from it and do some cleaning so that it would be usable for machine learning projects.\n",
    "\n",
    "### What can you do with this dataset?\n",
    "\n",
    "As described above, the original dataset of nearly 300 variables was reduced to just about 20 variables. In addition to classical EDA, this dataset can be used to apply a range of machine learning methods, most notably classifier models (logistic regression, SVM, random forest, etc.). You should treat the variable \"HeartDisease\" as a binary (\"Yes\" - respondent had heart disease; \"No\" - respondent had no heart disease). But note that classes are not balanced, so the classic model application approach is not advisable. Fixing the weights/undersampling should yield significantly betters results. Based on the dataset, I constructed a logistic regression model and embedded it in an application you might be inspired by: https://heart-condition-checker.herokuapp.com/. Can you indicate which variables have a significant effect on the likelihood of heart disease?\n",
    "\n",
    "### Data Dictionary\n",
    "\n",
    "The features available in the dataset are shown in the table below. The first variable, **HeartDisease**, is the target variable. We aim to predict whether **HeartDisease** is true or false. \n",
    "\n",
    "| Feature     | Description |\n",
    "| ----------- | ----------- |\n",
    "| **HeartDisease**       | Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI)    |\n",
    "| **BMI**   | Body Mass Index (BMI)        |\n",
    "| **Smoking** | Have you smoked at least 100 cigarettes in your entire life? |\n",
    "| **AlcoholDrinking** | Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week |\n",
    "| **Stroke** | Ever had a stroke? |\n",
    "| **PhysicalHealth** | Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 was your physical health not good. |\n",
    "| **MentalHealth** | Thinking about your mental health, for how many days during the past 30 days was your mental health not good? |\n",
    "| **DiffWalking** | Do you have serious difficulty walking or climbing stairs? |\n",
    "| **Sex** | Sex Assigned at Birth | \n",
    "| **AgeCategory** |  Fourteen-level age category |\n",
    "| **Race** | Race and ethnicity |\n",
    "| **Diabetic** | Have you ever had diabetes? |\n",
    "| **PhysicalActivity** | Adults who reported doing physical activity or exercise during the past 30 days other than their regular job |\n",
    "| **GenHealth** | Would you say that in general your health is...|\n",
    "| **SleepTime** | On average, how many hours of sleep do you get in a 24-hour period?|\n",
    "| **Asthma** | Have you ever had asthma?|\n",
    "| **KidneyDisease** | Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease? |\n",
    "| **SkinCancer** | Have you ever had skin cancer? |\n",
    "\n",
    "### What is our objective?\n",
    "\n",
    "Our objective is to use a variety demographic, health, and behavioral data to predict if the patients in this dataset have or ever had heart disease.\n",
    "\n",
    "### Import the Dataset\n",
    "\n",
    "We'll begin by importing the dataset and taking a look at the columns. Don't forget to refer to the data dictionary for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Import functions from scikit-learn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             classification_report,\n",
    "                             f1_score,\n",
    "                             precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     cross_val_predict,\n",
    "                                     StratifiedKFold,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     train_test_split)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (LabelEncoder,\n",
    "                                   OneHotEncoder,\n",
    "                                   StandardScaler)\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `pandas` to import the dataset. Be sure to use the correct file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/heart_2020_cleaned_sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 13 categorical features, 4 continuous featues, and one categorical target variable.\n",
    "\n",
    "Notice that we do not have any null values, so we can skip imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many null values per column?\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis and Feature Engineering\n",
    "\n",
    "Before we jump into modeling, it's important to get to know our data. This will help motivate the features we use, any additional features we construct, and how we perform preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Let's first get a sense of the distributions of the variables in the dataset. We'll start with the numerical data. Let's plot histograms of these features. Notice that, in some plots, we use a log-scale for the $y$-axis. Try turning it off to see how the distribution looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the numeric features\n",
    "df_numeric = df.select_dtypes(\"number\")\n",
    "numeric_features = df_numeric.columns\n",
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BMI\n",
    "sns.histplot(data=df_numeric, x='BMI', bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PhysicalHealth\n",
    "sns.histplot(data=df_numeric, x='PhysicalHealth', bins=10)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PhysicalHealth\n",
    "sns.histplot(data=df_numeric, x='MentalHealth', bins=10)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PhysicalHealth\n",
    "sns.histplot(data=df_numeric, x='SleepTime', bins=20)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 1: Correlation Plot\n",
    "\n",
    "Calculate the correlation matrix among the numeric features, and plot it using `seaborn`. Use this [example](https://seaborn.pydata.org/examples/many_pairwise_correlations.html) as a reference.\n",
    "\n",
    "You can create the correlation matrix with the `corr()` function, and then plot it using a `seaborn` `heatmap()`.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = df.select_dtypes(\"object\")\n",
    "categorical_features = df_categorical.columns\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the number of unique values for each categorical feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distributions of all the categorical features.\n",
    "\n",
    "Note that we're doing this in a single set of subplots using `matplotlib`. There's a lot of code here - don't stress too much about the details. Instead, focus on the plot output and what the distribution of the variables look like. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose 3 rows - feel free to adjust\n",
    "nrows = 3\n",
    "# Number of columns chosen automatically based on number of features\n",
    "ncols = categorical_features.size // 3 + 1\n",
    "\n",
    "# Create subplots using matplotlib\n",
    "fig, axes = plt.subplots(nrows=3, ncols=ncols, figsize=(nrows * 9, ncols * 2.5))\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "\n",
    "# Iterate over categorical features\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    # Choose axis for features\n",
    "    ax = axes[idx // ncols, idx % ncols]\n",
    "    # Calculate proportions and plot bars\n",
    "    df_categorical[feature].value_counts(normalize=True).sort_index().plot(\n",
    "        kind='bar',\n",
    "        ax=ax)\n",
    "    # Rotate x ticks\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "    # Set y limits\n",
    "    ax.set_ylim([0, 1])\n",
    "    # Create title for plot\n",
    "    ax.set_title(feature)\n",
    "\n",
    "# Adjustments to single plots\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=40, ha='right', fontsize=14)\n",
    "cur_xticks = axes[1, 2].get_xticklabels()\n",
    "cur_xticks[0] = 'AI/AN'\n",
    "axes[1, 2].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "axes[1, 1].set_ylim([0, 0.12])\n",
    "cur_xticks = axes[1, 3].get_xticklabels()\n",
    "cur_xticks[1] = 'Borderline'\n",
    "cur_xticks[3] = 'During\\nPregnancy'\n",
    "axes[1, 3].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "axes[2, 0].set_xticklabels(axes[2, 0].get_xticklabels(), rotation=40, ha='right')\n",
    "# Turn off unused plot\n",
    "axes[-1, -1].axis(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how these features correlation with the target variable.\n",
    "\n",
    "For the continuous variables, for example, we can examine the distribution of each feature separately for the samples where the patient had heart disease and the samples where the patient did not have heart disease.\n",
    "\n",
    "We can use a `seaborn` histogram with a `hue` argument to compare this directly. Pay attention to what arguments are passed into the function. What do these plots tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='BMI', hue='HeartDisease', stat='density', bins=50, common_norm=False)\n",
    "plt.xlim([0, 50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='PhysicalHealth', hue='HeartDisease', stat='density', bins=10, common_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='MentalHealth', hue='HeartDisease', stat='density', bins=10, common_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='SleepTime', hue='HeartDisease', stat='density', bins=15, common_norm=False)\n",
    "plt.xlim([0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the categorical data, we'll plot the average `HeartDisease` rate by each unique value of the variable. For example, consider how heart disease varies with smoking. Let's convert the heart disease features into a binary label to make this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable for heart disease\n",
    "df_categorical['HeartDiseaseBinary'] = np.where(df_categorical['HeartDisease'] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we group the samples by smoking, and calculate the heart disease rate for each group by averaging across the `HeartDiseaseBinary` variable. We can then visualize the rates as a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.groupby(\"Smoking\")['HeartDiseaseBinary'].mean().plot(kind = \"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this same procedure for all categorical features. Once again, don't worry too much about the code. Focus instead on what the data is telling you. What correlates with heart disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose 3 rows - feel free to adjust\n",
    "nrows = 3\n",
    "# Number of columns chosen automatically based on number of features\n",
    "categorical_predictors = [feature for feature in df_categorical.columns\n",
    "                          if 'HeartDisease' not in feature]\n",
    "ncols = len(categorical_predictors) // 3 + 1\n",
    "\n",
    "# Create subplots using matplotlib\n",
    "fig, axes = plt.subplots(nrows=3, ncols=ncols, figsize=(nrows * 9, ncols * 2.5))\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "\n",
    "# Iterate over categorical features\n",
    "for idx, feature in enumerate(categorical_predictors):\n",
    "    # Make sure we skip over the heart disease features\n",
    "    if 'HeartDisease' not in feature:\n",
    "        # Choose axis for features\n",
    "        ax = axes[idx // ncols, idx % ncols]\n",
    "        # Calculate proportions and plot bars\n",
    "        df_categorical.groupby(feature)['HeartDiseaseBinary'].mean().sort_index().plot(kind='bar', ax=ax)\n",
    "        # Remove x label\n",
    "        ax.set_xlabel('')\n",
    "        # Rotate x ticks\n",
    "        ax.tick_params(axis='x', rotation=0)\n",
    "        # Create title for plot\n",
    "        ax.set_title(feature)\n",
    "\n",
    "# Adjustments to single plots\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=40, ha='right', fontsize=14)\n",
    "cur_xticks = axes[1, 1].get_xticklabels()\n",
    "cur_xticks[0] = 'AI/AN'\n",
    "axes[1, 1].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "cur_xticks = axes[1, 2].get_xticklabels()\n",
    "cur_xticks[1] = 'Borderline'\n",
    "cur_xticks[3] = 'During\\nPregnancy'\n",
    "axes[1, 2].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "axes[1, 4].set_xticklabels(axes[1, 4].get_xticklabels(), rotation=40, ha='right')\n",
    "# Turn off unused plots\n",
    "axes[-1, -2].axis(False)\n",
    "axes[-1, -1].axis(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Preprocessing\n",
    "\n",
    "After we've performed EDA and gotten a sense of what features correlate with the target variable, we need to do feature engineering and preprocessing.\n",
    "\n",
    "Feature engineering is at the heart of machine learning, and there's no single way to do it. Feature engineering is the process of constructing new features that we think might be informative about the predictor variable. It could mean taking categorical data and one-hot encoding it, creating interaction terms, and preprocessing. These are all steps we take *prior* to fitting a model in order to make the data more suitable for prediction.\n",
    "\n",
    "We're going to do limited feature engineering in the interest of time. However, continue thinking about what useful features may exist while working with the data. Specifically, we will:\n",
    "\n",
    "- Adjust the age features,\n",
    "- Label encode the target variable,\n",
    "- Scale the numerical features,\n",
    "- One-hot encode the categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll adjust the age variable into a pseudo-continuous variable. We'll do this because the age category feature has 13 unique values, which is quite a lot for a categorical variable. Furthermore, age has an ordered structure, which we lose when we use the categorical formulation. What we'll do is replace each age value with the lower limit of the age range. We lose some information this way, but sometimes there's a cost to preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique age category values\n",
    "df[\"AgeCategory\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"Age\" column by taking the left number (remember, it's a string) and converting it to a float\n",
    "df[\"Age\"] = df['AgeCategory'].str[:2].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove the age category as well as the heart disease column to obtain a \"design matrix\". We'll also extract the heart disease column into its own dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"HeartDisease\", \"AgeCategory\"], axis=1).copy()\n",
    "y = df[\"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we scale and one hot encode data, let's first split it into training and validation datasets. Why do we do this? All preprocessing should be done separately on the training and test (or validation) set. We'll use `sklearn`'s `train_test_split` function to perform the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_valid_raw, y_train_raw, y_valid_raw = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we converted the heart disease feature into a binary feature using a `numpy` function. Now, we'll do it again, using a `scikit-learn` function. The benefit to using the `LabelEncoder` is that `scikit-learn` does all the work for us. It will also give us an object that can be applied to new data. For example, we can fit the `LabelEncoder` to the training data, and apply it to the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize label encoder\n",
    "labeler = LabelEncoder()\n",
    "# Fit and transform the target variable from the training set\n",
    "y_train = labeler.fit_transform(y_train_raw)\n",
    "# Transform the validation target variable\n",
    "y_valid = labeler.transform(y_valid_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What classes did we obtain?\n",
    "print(labeler.classes_)\n",
    "# Confidence check: does it work?\n",
    "print(labeler.transform([\"No\", \"Yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to perform additional preprocessing, we're going to create a `scikti-learn` `Pipeline`. The `Pipeline` allows us to compose multiple steps into a single object, which we can then fit and apply to multiple datasets. Let's take it one step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all features\n",
    "feature_cols = X_train_raw.columns\n",
    "# Identify numeric features\n",
    "numeric_cols = X_train_raw.select_dtypes(\"number\").columns.tolist()\n",
    "# Identify categorical features\n",
    "categorical_cols = X_train_raw.select_dtypes(\"object\").columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `Pipeline` is composed of steps. Each step is a tuple of two elements: the first tells us the name, and the second tells us what transformation is happening. We can create a `Pipeline` by stitching together steps via a list. We can also create a `Pipeline` by stitching together smaller `Pipeline`s.\n",
    "\n",
    "The tricky thing about a `Pipeline` is that it applies a transformation to all the data. This won't work in cases with heterogeneous data. For example, we don't want to one-hot encode continuous features, and we don't want to standardize categorical features. So, we need one more tool: the `ColumnTransformer`. The `ColumnTransformer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pipeline to create a numeric transformer: the Pipeline accepts a list of tuples\n",
    "numeric_transformer = Pipeline([(\"scaler\", StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pipeline to create a categorical transformer\n",
    "categorical_transformer = Pipeline(\n",
    "    [(\"one_hot_encoder\",\n",
    "      OneHotEncoder(categories='auto', \n",
    "                    handle_unknown='error', \n",
    "                    sparse=False,\n",
    "                    drop=\"first\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we create the overall preprocessor with the ColumnTransformer.\n",
    "# The ColumnTransformer is itself a Pipeline, and needs steps (i.e., a list). \n",
    "# In this case, each step needs a tuple with length 3:\n",
    "# 1. The name of the step.\n",
    "# 2. The Pipeline to apply at that step.\n",
    "# 3. The columns to apply the step to.\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    # First step: numeric features\n",
    "    (\"numeric\", numeric_transformer, numeric_cols),\n",
    "    # Second step: categorical features\n",
    "    (\"categorical\", categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines can also have classifiers (e.g., a `LogisticRegression`) included as well. In that case, the output of the pipeline would be a trained model. For now, however, we'll simply just preprocess the data using the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transform the train dataset using the pipeline\n",
    "X_train = preprocessor.fit_transform(X_train_raw)\n",
    "# Transform the testing dataset with the rules learned from the training dataset\n",
    "X_valid = preprocessor.transform(X_valid_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our data preprocessed. Notice how easy, clean, and reproducible this process was. This demonstrates the value of using the `Pipeline` to conduct machine learning analyses. We can quickly and cleanly transform any new batches of data to confirm to the rules established by the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View result\n",
    "print(X_train.shape)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 2: Pipelines on Training vs. Validation Data\n",
    "\n",
    "Why do we use `fit_transform` for the training data, and only `transform` for the validation data? How does this prevent data leakage?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 3: Extending Pipelines\n",
    "\n",
    "A common preprocessing step is to decorrelate data via Principal Components Analysis (PCA). We have an alternative workshop on PCA and other dimensionality reduction techniques, but for now, all you need to know is that it has its own transformation object in `scitkit-learn`, just like the `StandardScaler`. Extend the `numeric_transformer` to have an additional step which performs PCA. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) on PCA in `scikit-learn`.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of the preprocessor is a `numpy` array with more columns than the original dataframe. This is because the one-hot encoding created some new columns. It'd be nice if we had this data in a data frame, with named columns. So, the last step we'll do is convert this back into a data frame. First, we need to access the new column names, which we can do with the `get_feature_names_out` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.named_transformers_['categorical']['one_hot_encoder'].get_feature_names_out(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access pipeline data to so we can name the one-hot encoded columns\n",
    "new_categorical_cols = preprocessor.named_transformers_['categorical']['one_hot_encoder'].get_feature_names_out(categorical_cols).tolist()\n",
    "# Create list of column names - numeric columns don't change\n",
    "column_names = numeric_cols + new_categorical_cols\n",
    "# Create dataframes\n",
    "X_train = pd.DataFrame(data=X_train, columns=column_names)\n",
    "X_valid = pd.DataFrame(data=X_valid, columns=column_names)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We have our finalized dataset and we can move on to building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Modeling Proccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun begins!\n",
    "\n",
    "But first: we need to calculate a baseline accuracy. This is the fraction of the data that is of one particular class. We can calculate this by taking the mean of the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 4: Baseline Accuracy\n",
    "\n",
    "Calculate the baseline accuracy of the training data. What does this baseline accuracy mean in the context of the data? What does it mean in the context of trying to predict heart disease outcomes?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 5: False Positives and False Negatives\n",
    "\n",
    "Accuracy is not the only thing we need to be worried about when building machine learning models. A model can be accurate, and still have issues in what samples it classifies correctly, and what samples it makes mistakes on.\n",
    "\n",
    "For example, consider false positives and false negatives. What do each mean in this context? To be clear, if a model produces a false positive, what does that mean in the context of classifying heat disease?\n",
    "\n",
    "If such a model were deployed in real life, which of the two - false positives or false negatives - would be more costly? Which should we be more concerned about?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Logistic Regression\n",
    "\n",
    "The first model we'll try is called logistic regression, which we've already studied in Python Machine Learning. As a reminder, logistic regression is a linear model that can be used to predict the probability of a sample falling in a certain class. Thus, it's a common model for classification.\n",
    "\n",
    "We're going to use the `cross_val_score` function to calculate model performance across folds in the training data. The way we'll cross-validate is via the `StatifiedKFold` cross-validator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 6: Stratifying Cross-Validation\n",
    "\n",
    "What does `StratifiedKFold` do that `KFold` does not? Why is stratifying cross-validation important, particularly in this context?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of metric functions\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validator\n",
    "skfold = StratifiedKFold(n_folds)\n",
    "# Create model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Iterate over metrics\n",
    "for metric in metrics:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=skfold, scoring=metric)\n",
    "    print(f\"Mean {metric} score is {cv_results.mean().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 7: Ridge Regression\n",
    "\n",
    "Re-run the above analysis, but use ridge regression instead. In particular, use `RidgeCV` so that you can choose the best regularization penalty. \n",
    "\n",
    "Notice that, by doing this, we have two loops of cross-validation: an outer loop in which we report metrics, and an inner loop which we use to choose the regularization penalty. This is a common way to report generalization accuracy.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, the accuracy score of 91.6% might look pretty good. However, recall our baseline accuracy. The baseline accuracy is what we could get if we simply said *nobody* got heart disease. Does our accuracy score look good in that context? This is why establishing baseline accuracy is important, particularly in imbalanced data!\n",
    "\n",
    "What about the precision and recall scores? What do those metrics tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: Decision Trees\n",
    "\n",
    "Next, let's try using a decision tree. We studied these in Part 1 of Machine Learning Fundamentals. You may recall that decision trees have a wide array of *hyperparameters*, or settings in the model we set before fitting it to the data. These can include the maximum depth, the criterion used for performing a split, etc. When we first fit the decision tree, we used default parameters for all of these specified by `scitkit-learn`. How can we go about *choosing* the best values instead?\n",
    "\n",
    "In the case of ridge regression, we did a cross-validation procedure to choose the best hyperparameter. When we have many hyperparameters though, we'll need to do cross-validation across all combinations. Enter the **grid search**, which we can use to search across all combinations of hyperparameters to find the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Model Selection\n",
    "\n",
    "Grid search is a brute-force method that executes cross-validation for *all* possible combinations of hyperparameters from a set of hyperparameter ranges.\n",
    "\n",
    "Let's consider an example. Suppose we have two hyperparameters $A$ and $B$. We don't know what values to choose for them, so we'll use a grid search to identify the best set. Grid search requires we specify hyperparameter ranges. So, let's say hyperparameter $A$ can be either of the two values $(0, 1)$, and hyperparameter $B$ can be either of the two values $(2, 3)$ (in practice, we might choose more values, but we'll use two each for simplicity). \n",
    "\n",
    "Grid search forms each combination of hyperparameters, and fits a model for it. We can then use the valiation performance to choose the best combination across all hyperparameters. In this case, we'd consider all the following combinations:\n",
    "\n",
    "- $A = 0$, $B = 1$\n",
    "- $A = 0$, $B = 3$\n",
    "- $A = 2$, $B = 1$\n",
    "- $A = 2$, $B = 3$\n",
    "\n",
    "and choose the combination that performs the best.\n",
    "\n",
    "We can easily perform this process by using `scikit-learn`'s `GridSearchCV`. Let's take a look at how it works by running it on the `max_depth` and `min_samples_leaf` hyperparameters in a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify a parameter grid as a dictionary\n",
    "param_grid = {\n",
    "    \"max_depth\": np.arange(5, 50, 5),\n",
    "    \"min_samples_leaf\": np.arange(20, 200, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the size of the parameter grid we're tuning on?\n",
    "param_grid[\"max_depth\"].shape[0] * param_grid[\"min_samples_leaf\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 162 different sets of parameters. That's a lot of models to fit!\n",
    "\n",
    "Next, we pass some information into the `GridSearchCV` object (check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt = GridSearchCV(\n",
    "    # Specify the model\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    # Specify the hyperparameter grid\n",
    "    param_grid=param_grid,\n",
    "    # What metric should we use to select for the best model?\n",
    "    scoring = \"accuracy\",\n",
    "    # How do we generate cross-validation folds?\n",
    "    cv=skfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can fit the grid search. Let's use the `%%time` magic command to see how long this process takes. \n",
    "\n",
    "Note that `%%time` is a magic command to measure the time of execution for a whole cell, whereas `%time` only measures the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit the grid search object on the training data\n",
    "grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a fitted grid search variable! Let's take a look at what we get with it. First, the best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the best cross-validated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search variable is its own predictor, and we can run it on any set of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also directly access the estimator used in the `predict()` function. This is the `best_estimator_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = grid_dt.best_estimator_\n",
    "type(best_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 8: Choosing a Different Scoring Metric\n",
    "\n",
    "Run a new grid search, this time using recall as the choice of the scoring metric. What are the best parameters in this case? Are they different from before?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: Random Forests\n",
    "\n",
    "So far, our modeling hasn't yield great results. Let's consider a different model, which is more commonly used in harder prediction problems.\n",
    "\n",
    "This model is called the Random Forest. As you might expect from the name, a random forest is a collection of many decision trees. Specifically, it's an **ensemble model**, since it consists of an ensemble of $N$ decision trees. The $N$ tree in the forest can separately make predictions, each of which counts as a vote toward the final prediction. The ensemble prediction - typically by majority voting - performs better than a single tree alone. This is the machine learning version of a model \"greater than the sum of its parts\".\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few important things to note about the random forest:\n",
    "\n",
    "- Each tree in the forest is not trained on the same data and features. That would be counterproductive, because you would up end up with dozens of duplicate trees.\n",
    "- Instead, the trees are trained on a subset (usually a random 2/3) of the features and a bootstrapped sample of the data. This helps reduce the variance of the predictions.\n",
    "- To further decorrelate the trees, pruning trees is discouraged for the purpose of overfitting.\n",
    "\n",
    "![](https://miro.medium.com/max/1240/1*EemYMyOADnT0lJWSXmTDdg.jpeg)\n",
    "\n",
    "We are going to gloss over some of the details of the random forest in order to focus on their application in this context. However, those details are important! Check out this [blog post](https://victorzhou.com/blog/intro-to-random-forests/) for a gentle introduction to random forests. For a *very* in-depth explanation of random forests, check out Chapter 15 of [Elements of Statistical Learning Theory](https://hastie.su.domains/Papers/ESLII.pdf).\n",
    "\n",
    "Let's get a sense for how a random forest performs without any hyperparameter tuning. We'll use the `RandomForestClassifier` from `scikit-learn`. The `n_estimators` argument is where we specify the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest\n",
    "rf = RandomForestClassifier(n_estimators=50)\n",
    "# Cross-validate\n",
    "cv_results = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much improvement. Let's bring in the grid search to see if we can improve on this result.\n",
    "\n",
    "Let's try varying the following hyperparameters: `n_estimators` and `min_samples_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"min_samples_split\": [2, 5, 10, 0.25],\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=skfold,\n",
    "    scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did slightly better than the baseline accuracy, but not much - this is a hard problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 8: Random Search\n",
    "\n",
    "The downside of grid search is that it can take a long time, especially when you have a large number hyperparameters, a complex model, and a lot of data. Grid search quickly becomes unwieldy because \n",
    "\n",
    "RandomSearchCV is a solution to this issue because. In a random search, we randomly select a fraction of the hyperparameters sets to evaluate model prformance. In a random search, you're not guaranteed to find the best set of parameters. However, it oftens performs pretty well, especially when there are computational constraints.\n",
    "\n",
    "You can do a random search in `scikit-learn` with `RandomSearchCV`. Choose a set of hyperparameters, and run a random search with `RandomSearchCV`. What are the best set of parameters?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation and Interpretation\n",
    "\n",
    "It's time to bring back our validation dataset to get a stronger sense of how well our models perform on out-of-sample data.\n",
    "\n",
    "We're going to use the logistic regression, decision tree, and random forest models we created to make predictions on the validation features and evaluate them on a variety of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit logisic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "lr_pred = lr.predict(X_valid)\n",
    "dt_pred = best_dt.predict(X_valid)\n",
    "rf_pred = best_rf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `classification_report` and `confusion_matrix` functions to evaluate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression\\n')\n",
    "print(confusion_matrix(y_valid, lr_pred))\n",
    "print(classification_report(y_valid, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree\\n')\n",
    "print(confusion_matrix(y_valid, dt_pred))\n",
    "print(classification_report(y_valid, dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest\\n')\n",
    "print(confusion_matrix(y_valid, rf_pred))\n",
    "print(classification_report(y_valid, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do all the models compare to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The model performance we obtained was OK, but not amazing. This often happens in the development of machine learning algorithms. It's useful at this point to try and interpret our models to see where they're getting signal from in order to decide what to do next. Do we need more data? Do we need better features? Do we need a better model?\n",
    "\n",
    "First, let's take a look at the logistic regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr.coef_[0]\n",
    "coefs = pd.Series(index=column_names, data = coefs)\n",
    "coefs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features, according to the model, are the most and least associated with heart disease? How should you interpret categorical coefficients versus numerical coefficients? What do the sign of the coefficients mean?\n",
    "\n",
    "With tree-based models, feature importance works a little differently. We can access a `feature_importannces_` attribute which captures the \"importance\", defined as \"The (normalized) total reduction of the criterion brought by that feature.\" Basically, a quantification of how much the criterion we used (in our case the Gini impurity) was impacted by the feature's decision point. Notice that these feature importances are not signed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fi = best_dt.feature_importances_\n",
    "dt_fi = pd.Series(index=column_names, data=dt_fi)\n",
    "dt_fi.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fi = best_rf.feature_importances_\n",
    "rf_fi = pd.Series(index=column_names, data=rf_fi)\n",
    "rf_fi.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two sets of feature importances differ greatly? What do they tell us about predicting heart disease?\n",
    "\n",
    "Some of the feature importances are pretty low. This could imply that we should cut them out of the model. This may improve generalization performance, since the model is not trying to incorporate those less predictive features during training. This choice falls in the domain of *feature selection*. So, in future work, one thing we could do is retrain models without these features. We could also use regularization to implicitly do feature selection (e.g., a Lasso regression).\n",
    "\n",
    "Try to think about steps you might take to improve your models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Walkthrough Recap\n",
    "\n",
    "In this exercise we attempted to predict the onset of heart disease. We did the following:\n",
    "\n",
    "- We familiarized ourselves with the data and its patterns by studying the data dictionary and conducting exploratory data analyses.\n",
    "- We applied a number of feature engineering techniques to the data to prepare for modeling.\n",
    "- We employed three different machine learning models, two of which we parameter tuned in order maximize the generalization performance.\n",
    "- We evaluated our models on a validation dataset to get a sense of how well it does on a out-of-sample dataset.\n",
    "- We analyzed the relationship between the features and target variable by using attributes provided by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
